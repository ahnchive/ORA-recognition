{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bfb98a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# load required libraries & modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import pprint\n",
    "import time\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from utils import *\n",
    "from loaddata import *\n",
    "from visualization import *\n",
    "from rrcapsnet_original import *\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "\n",
    "DATA_DIR = '../data'\n",
    "DEVICE = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "# DEVICE = torch.device('cpu')\n",
    "\n",
    "BATCHSIZE = 1000\n",
    "\n",
    "PATH_MNISTC = '../data/MNIST_C/'\n",
    "CORRUPTION_TYPES = ['identity', \n",
    "         'shot_noise', 'impulse_noise','glass_blur','motion_blur',\n",
    "         'shear', 'scale',  'rotate',  'brightness',  'translate',\n",
    "         'stripe', 'fog','spatter','dotted_line', 'zigzag',\n",
    "         'canny_edges']\n",
    "\n",
    "N_MINI_PER_CORRUPTION = 1000\n",
    "\n",
    "ACC_TYPE = \"entropy\"\n",
    "\n",
    "# general helper funtions for model testing\n",
    "def load_model(args):\n",
    "    # load model\n",
    "    model = RRCapsNet(args).to(args.device) \n",
    "    model.load_state_dict(torch.load(args.load_model_path))\n",
    "    return model\n",
    "\n",
    "def load_args(load_model_path, args_to_update, verbose=False):\n",
    "    params_filename = os.path.dirname(load_model_path) + '/params.txt'\n",
    "    assert os.path.isfile(params_filename), \"No param flie exists\"\n",
    "    args = parse_params_wremove(params_filename, removelist = ['device']) \n",
    "    args = update_args(args, args_to_update)\n",
    "    args.load_model_path = load_model_path\n",
    "    if verbose:\n",
    "        pprint.pprint(args.__dict__, sort_dicts=False)\n",
    "    return args\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_on_batch(task, batchnum, model, args, train=False, verbose=True, onlyacc=False):\n",
    "    \n",
    "    # evaluate on one train/test batch \n",
    "    model.eval()\n",
    "    \n",
    "    if task == 'mnist_recon':\n",
    "        # for mnist recon data, it has erased input(x) and intact input (gtx)\n",
    "        if train:\n",
    "            dataloader, val_dataloader = fetch_dataloader(task, DATA_DIR, DEVICE, BATCHSIZE, train)\n",
    "        else:\n",
    "            dataloader = fetch_dataloader(task, DATA_DIR, DEVICE, BATCHSIZE, train)            \n",
    "        diter = iter(dataloader)\n",
    "        for i in range(batchnum):\n",
    "            x, gtx, y = next(diter)\n",
    "    else:\n",
    "        dataloader = fetch_dataloader(task, DATA_DIR, DEVICE, BATCHSIZE, train)    \n",
    "        diter = iter(dataloader)\n",
    "        for i in range(batchnum):\n",
    "            x, y = next(diter)\n",
    "            gtx = None\n",
    "            \n",
    "            \n",
    "    # attach forward hooks for intermediate outputs for visualizations\n",
    "    outputs = {}\n",
    "    \n",
    "    # from model main output\n",
    "    x_input_step = []; x_mask_step = []; objcaps_step = []\n",
    "    \n",
    "    # from model dynamic routing\n",
    "    coups_step = []; betas_step= []; rscores_step=[]; recon_coups_step=[] \n",
    "    outcaps_len_step=[]; outcaps_len_before_step=[]\n",
    "\n",
    "    def get_attention_outputs():\n",
    "        def hook(model, input, output):\n",
    "            x_mask_step.append(output[0].detach())\n",
    "            x_input_step.append(output[1].detach())\n",
    "        return hook\n",
    "\n",
    "    def get_capsule_outputs():\n",
    "        def hook(model, input, output):\n",
    "            objcaps_step.append(output[0].detach())\n",
    "            coups_step.append(torch.stack(output[1]['coups'], dim=1))\n",
    "            betas_step.append(torch.stack(output[1]['betas'], dim=1)) \n",
    "            if 'rscores' in output[1].keys():\n",
    "                rscores_step.append(torch.stack(output[1]['rscores'], dim=1))\n",
    "            if 'recon_coups' in output[1].keys():\n",
    "                recon_coups_step.append(torch.stack(output[1]['recon_coups'], dim=1))\n",
    "            if 'outcaps_len' in output[1].keys():\n",
    "                outcaps_len_step.append(torch.stack(output[1]['outcaps_len'], dim=1))\n",
    "            if 'outcaps_len_before' in output[1].keys():\n",
    "                outcaps_len_before_step.append(torch.stack(output[1]['outcaps_len_before'], dim=1))\n",
    "        return hook\n",
    "    \n",
    "\n",
    "    hook1 = model.input_window.register_forward_hook(get_attention_outputs())\n",
    "    hook2 = model.capsule_routing.register_forward_hook(get_capsule_outputs())\n",
    "    \n",
    "    # evaluate and detach hooks\n",
    "    losses, acc, objcaps_len_step, x_recon_step = evaluate(model, x, y, args, acc_type=ACC_TYPE, gtx=gtx)\n",
    "    hook1.remove()\n",
    "    hook2.remove()\n",
    "    \n",
    "    # add tensor outputs dictionary\n",
    "    outputs['x_input'] = torch.stack(x_input_step, dim=1)\n",
    "    outputs['x_mask'] = torch.stack(x_mask_step, dim=1)\n",
    "    outputs['objcaps'] = torch.stack(objcaps_step, dim=1)\n",
    "    \n",
    "    outputs['coups'] = torch.stack(coups_step, dim=1)\n",
    "    outputs['betas'] = torch.stack(betas_step, dim=1)\n",
    "    if rscores_step:\n",
    "        outputs['rscores'] = torch.stack(rscores_step, dim=1)\n",
    "    if recon_coups_step:\n",
    "        outputs['recon_coups'] = torch.stack(recon_coups_step, dim=1)\n",
    "    if outcaps_len_step:\n",
    "        outputs['outcaps_len'] = torch.stack(outcaps_len_step, dim=1)\n",
    "    if outcaps_len_before_step:\n",
    "        outputs['outcaps_len_before'] = torch.stack(outcaps_len_before_step, dim=1)        \n",
    "    if verbose:\n",
    "        print(\"==> On this sigle test batch: test_loss=%.5f, test_loss_class=%.5f, test_loss_recon=%.5f, test_acc=%.4f\"\n",
    "              % (losses[0], losses[1], losses[2], acc))\n",
    "    \n",
    "    if onlyacc:\n",
    "        return acc\n",
    "    else:\n",
    "        return x, gtx, y, objcaps_len_step, x_recon_step, outputs\n",
    "    \n",
    "    \n",
    "@torch.no_grad()\n",
    "def evaluate_model_on_mnistc_mini(corruption, model, args, train=False, verbose=True, save_hooks=False, max_batch_num=None):\n",
    "    \n",
    "    # get corruption batch information\n",
    "    corruption_id = int(CORRUPTION_TYPES.index(corruption))\n",
    "    num_batch_required = int(N_MINI_PER_CORRUPTION/BATCHSIZE) # if batchsize 100; 10 batches are requried\n",
    "    \n",
    "    # load dataloader and iterator\n",
    "    dataloader = fetch_dataloader('mnist_c_mini', DATA_DIR, DEVICE, BATCHSIZE, train)    \n",
    "    diter = iter(dataloader)\n",
    "\n",
    "    if save_hooks:\n",
    "        def get_attention_outputs():\n",
    "            def hook(model, input, output):\n",
    "                x_mask_step.append(output[0].detach())\n",
    "                x_input_step.append(output[1].detach())\n",
    "            return hook\n",
    "\n",
    "        def get_capsule_outputs():\n",
    "            def hook(model, input, output):\n",
    "                objcaps_step.append(output[0].detach())\n",
    "                coups_step.append(torch.stack(output[1]['coups'], dim=1))\n",
    "                betas_step.append(torch.stack(output[1]['betas'], dim=1)) \n",
    "                if 'rscores' in output[1].keys():\n",
    "                    rscores_step.append(torch.stack(output[1]['rscores'], dim=1))\n",
    "                if 'recon_coups' in output[1].keys():\n",
    "                    recon_coups_step.append(torch.stack(output[1]['recon_coups'], dim=1))\n",
    "                if 'outcaps_len' in output[1].keys():\n",
    "                    outcaps_len_step.append(torch.stack(output[1]['outcaps_len'], dim=1))\n",
    "                if 'outcaps_len_before' in output[1].keys():\n",
    "                    outcaps_len_before_step.append(torch.stack(output[1]['outcaps_len_before'], dim=1))\n",
    "            return hook\n",
    "        \n",
    "        outputs = {}\n",
    "\n",
    "        x_input_step_all = []; x_mask_step_all = []; objcaps_step_all = []\n",
    "\n",
    "        coups_step_all = []; betas_step_all= []; rscores_step_all=[]; recon_coups_step_all=[] \n",
    "        outcaps_len_step_all=[]; outcaps_len_before_step_all=[]\n",
    "\n",
    "    x_all, y_all, gtx_all, loss_all, acc_all, objcaps_len_step_all, x_recon_step_all = [],[],[],[],[],[],[]\n",
    "    model.eval()        \n",
    "    # get input and gt\n",
    "    for i in range(corruption_id*num_batch_required): #id =0, 0 iteration; id=1, 10 iteration\n",
    "        x, y = next(diter)\n",
    "        gtx = None\n",
    "    \n",
    "    for i in range(0, num_batch_required):\n",
    "        x, y = next(diter)\n",
    "        gtx = None\n",
    "        \n",
    "        if max_batch_num:\n",
    "            if i == max_batch_num:\n",
    "                break\n",
    "            \n",
    "        # for hooks over other model output\n",
    "        x_input_step = []; x_mask_step = []; objcaps_step = []\n",
    "\n",
    "        if save_hooks:\n",
    "\n",
    "            # for hooks over dynamic routing\n",
    "            coups_step = []; betas_step= []; rscores_step=[]; recon_coups_step=[] \n",
    "            outcaps_len_step=[]; outcaps_len_before_step=[]\n",
    "\n",
    "            hook1 = model.input_window.register_forward_hook(get_attention_outputs())\n",
    "            hook2 = model.capsule_routing.register_forward_hook(get_capsule_outputs())\n",
    "\n",
    "        # evaluate and append results \n",
    "        losses, acc, objcaps_len_step, x_recon_step = evaluate(model, x, y, args, acc_type=ACC_TYPE, gtx=gtx)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"==> On this sigle test batch: test_loss=%.5f, test_loss_class=%.5f, test_loss_recon=%.5f, test_acc=%.4f\"\n",
    "                  % (losses[0], losses[1], losses[2], acc))   \n",
    "            \n",
    "        # main input and output append\n",
    "        x_all.append(x)\n",
    "        y_all.append(y)\n",
    "        if gtx:\n",
    "            gtx_all.append(gtx)\n",
    "        #         loss_all.append(losses[0])\n",
    "        acc_all.append(acc)\n",
    "        objcaps_len_step_all.append(objcaps_len_step)\n",
    "        x_recon_step_all.append(x_recon_step)\n",
    "        \n",
    "        if save_hooks:\n",
    "        \n",
    "            # hook variables append\n",
    "            x_input_step_all.append(torch.stack(x_input_step, dim=1))\n",
    "            x_mask_step_all.append(torch.stack(x_mask_step, dim=1))\n",
    "            objcaps_step_all.append(torch.stack(objcaps_step, dim=1))\n",
    "\n",
    "            coups_step_all.append(torch.stack(coups_step, dim=1))\n",
    "            betas_step_all.append(torch.stack(betas_step, dim=1))\n",
    "            if rscores_step:\n",
    "                rscores_step_all.append(torch.stack(rscores_step, dim=1))\n",
    "            if recon_coups_step:\n",
    "                recon_coups_step_all.append(torch.stack(recon_coups_step, dim=1))\n",
    "            if outcaps_len_step:\n",
    "                outcaps_len_step_all.append(torch.stack(outcaps_len_step, dim=1))\n",
    "            if outcaps_len_before_step:\n",
    "                outcaps_len_before_step_all.append(torch.stack(outcaps_len_before_step, dim=1))\n",
    "\n",
    "            hook1.remove()\n",
    "            hook2.remove()\n",
    "    \n",
    "        \n",
    "    # concat and add to outputs dictionary\n",
    "    x_all = torch.cat(x_all, dim=0)\n",
    "    y_all = torch.cat(y_all, dim=0)\n",
    "    if gtx:\n",
    "        gtx_all = torch.cat(gtx_all, dim=0)\n",
    "    else:\n",
    "        gtx_all = gtx\n",
    "    acc_all = torch.cat(acc_all, dim=0)\n",
    "    objcaps_len_step_all = torch.cat(objcaps_len_step_all, dim=0)\n",
    "    x_recon_step_all = torch.cat(x_recon_step_all, dim=0)\n",
    "    \n",
    "    if save_hooks:\n",
    "        outputs['x_input']= torch.cat(x_input_step_all, dim=0)\n",
    "        outputs['x_mask']= torch.cat(x_mask_step_all, dim=0)\n",
    "        outputs['objcaps']= torch.cat(objcaps_step_all, dim=0)\n",
    "\n",
    "        outputs['coups'] = torch.cat(coups_step_all, dim=0)\n",
    "        outputs['betas'] = torch.cat(betas_step_all, dim=0)\n",
    "        if rscores_step_all:\n",
    "            outputs['rscores'] = torch.cat(rscores_step_all, dim=0)\n",
    "        if recon_coups_step_all:\n",
    "            outputs['recon_coups'] = torch.cat(recon_coups_step_all, dim=0)\n",
    "        if outcaps_len_step_all:\n",
    "            outputs['outcaps_len'] = torch.cat(outcaps_len_step_all, dim=0)\n",
    "        if outcaps_len_before_step_all:\n",
    "            outputs['outcaps_len_before'] = torch.cat(outcaps_len_before_step_all, dim=0)\n",
    "            \n",
    "        return x_all, gtx_all, y_all, acc_all, objcaps_len_step_all, x_recon_step_all, outputs  \n",
    "\n",
    "    else:\n",
    "        return x_all, gtx_all, y_all, acc_all, objcaps_len_step_all, x_recon_step_all\n",
    "    \n",
    "    \n",
    "@torch.no_grad()\n",
    "def evaluate_cnn_on_mnistc_mini(corruption, cnn, max_batch_num=None):\n",
    "    # get corruption batch information\n",
    "    corruption_id = int(CORRUPTION_TYPES.index(corruption))\n",
    "    num_batch_required = int(N_MINI_PER_CORRUPTION/BATCHSIZE) # if batchsize 100; 10 batches are requried\n",
    "    \n",
    "    # load dataloader and iterator\n",
    "    dataloader = fetch_dataloader('mnist_c_mini', DATA_DIR, DEVICE, BATCHSIZE, train=False)    \n",
    "    diter = iter(dataloader)\n",
    "    \n",
    "    # save output\n",
    "    x_all, y_all, pred_all, acc_all, class_prob_all = [],[],[], [],[]\n",
    "    cnn.eval() \n",
    "\n",
    "    # get input and gt\n",
    "    for i in range(corruption_id*num_batch_required): #id =0, 0 iteration; id=1, 10 iteration\n",
    "        x, y = next(diter)\n",
    "    \n",
    "\n",
    "    for i in range(0, num_batch_required):\n",
    "        x, y = next(diter)\n",
    "        gtx = None\n",
    "        \n",
    "        if max_batch_num:\n",
    "            if i == max_batch_num:\n",
    "                break\n",
    "        \n",
    "        data, target = x.to(DEVICE),  y.to(DEVICE)\n",
    "        target = target.argmax(dim=1, keepdim=True)\n",
    "        output = cnn(data)\n",
    "        #                 test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        acc = pred.eq(target.view_as(pred))\n",
    "        \n",
    "        x_all.append(data)\n",
    "        y_all.append(target.flatten())\n",
    "        pred_all.append(pred.flatten())\n",
    "        acc_all.append(acc.flatten().float())\n",
    "        class_prob_all.append(output)\n",
    "    \n",
    "\n",
    "    # concat and add to outputs dictionary\n",
    "    x_all = torch.cat(x_all, dim=0)\n",
    "    y_all = torch.cat(y_all, dim=0)\n",
    "    pred_all = torch.cat(pred_all, dim=0)\n",
    "    acc_all = torch.cat(acc_all, dim=0)\n",
    "    class_prob_all = torch.cat(class_prob_all, dim=0)\n",
    "\n",
    "    return x_all, y_all, class_prob_all, pred_all, acc_all\n",
    "    \n",
    "@torch.no_grad()\n",
    "def evaluate_model_on_mnistc_original(corruption, model, verbose=False, save_hooks=False,  max_batch_num=None):\n",
    "    path_images = os.path.join(PATH_MNISTC, corruption, 'test_images.npy')\n",
    "    path_labels = os.path.join(PATH_MNISTC, corruption, 'test_labels.npy')\n",
    "\n",
    "    # convert to torch\n",
    "    images = np.load(path_images)\n",
    "    labels = np.load(path_labels)\n",
    "    transform_tohot = T.Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "    images_tensorized = torch.stack([T.ToTensor()(im) for im in images])\n",
    "    labels_tensorized = torch.stack([transform_tohot(label) for label in labels])\n",
    "    # print(images_tensorized.shape) #torch.Size([10000, 1, 28, 28])\n",
    "    # print(labels_tensorized.shape) #torch.Size([10000, 10])\n",
    "\n",
    "    # create dataloader\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if DEVICE == 'cuda' else {}\n",
    "    dataset = TensorDataset(images_tensorized, labels_tensorized)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCHSIZE, shuffle=False, drop_last=False, **kwargs)\n",
    "\n",
    "    if save_hooks:\n",
    "        def get_attention_outputs():\n",
    "            def hook(model, input, output):\n",
    "                x_mask_step.append(output[0].detach())\n",
    "                x_input_step.append(output[1].detach())\n",
    "            return hook\n",
    "\n",
    "        def get_capsule_outputs():\n",
    "            def hook(model, input, output):\n",
    "                objcaps_step.append(output[0].detach())\n",
    "                coups_step.append(torch.stack(output[1]['coups'], dim=1))\n",
    "                betas_step.append(torch.stack(output[1]['betas'], dim=1)) \n",
    "                if 'rscores' in output[1].keys():\n",
    "                    rscores_step.append(torch.stack(output[1]['rscores'], dim=1))\n",
    "                if 'recon_coups' in output[1].keys():\n",
    "                    recon_coups_step.append(torch.stack(output[1]['recon_coups'], dim=1))\n",
    "                if 'outcaps_len' in output[1].keys():\n",
    "                    outcaps_len_step.append(torch.stack(output[1]['outcaps_len'], dim=1))\n",
    "                if 'outcaps_len_before' in output[1].keys():\n",
    "                    outcaps_len_before_step.append(torch.stack(output[1]['outcaps_len_before'], dim=1))\n",
    "            return hook\n",
    "        \n",
    "        outputs = {}\n",
    "\n",
    "        x_input_step_all = []; x_mask_step_all = []; objcaps_step_all = []\n",
    "\n",
    "        coups_step_all = []; betas_step_all= []; rscores_step_all=[]; recon_coups_step_all=[] \n",
    "        outcaps_len_step_all=[]; outcaps_len_before_step_all=[]\n",
    "\n",
    "    x_all, y_all, gtx_all, loss_all, acc_all, objcaps_len_step_all, x_recon_step_all = [],[],[],[],[],[],[]\n",
    "    \n",
    "    model.eval()      \n",
    "    \n",
    "    # get input and gt\n",
    "    i=0\n",
    "    for data in dataloader:\n",
    "        \n",
    "#         if max_batch_num:\n",
    "#             if i == max_batch_num:\n",
    "#                 break\n",
    "        if i == max_batch_num:\n",
    "            x, y = data\n",
    "            gtx = None\n",
    "\n",
    "            # for hooks over other model output\n",
    "            x_input_step = []; x_mask_step = []; objcaps_step = []\n",
    "\n",
    "            if save_hooks:\n",
    "\n",
    "                # for hooks over dynamic routing\n",
    "                coups_step = []; betas_step= []; rscores_step=[]; recon_coups_step=[] \n",
    "                outcaps_len_step=[]; outcaps_len_before_step=[]\n",
    "\n",
    "                hook1 = model.input_window.register_forward_hook(get_attention_outputs())\n",
    "                hook2 = model.capsule_routing.register_forward_hook(get_capsule_outputs())\n",
    "\n",
    "            # evaluate and append results \n",
    "            losses, acc, objcaps_len_step, x_recon_step = evaluate(model, x, y, args, acc_type=ACC_TYPE, gtx=gtx)\n",
    "\n",
    "            if verbose:\n",
    "                print(\"==> On this sigle test batch: test_loss=%.5f, test_loss_class=%.5f, test_loss_recon=%.5f, test_acc=%.4f\"\n",
    "                      % (losses[0], losses[1], losses[2], acc))   \n",
    "\n",
    "            # main input and output append\n",
    "            x_all.append(x)\n",
    "            y_all.append(y)\n",
    "            if gtx:\n",
    "                gtx_all.append(gtx)\n",
    "            #         loss_all.append(losses[0])\n",
    "            acc_all.append(acc)\n",
    "            objcaps_len_step_all.append(objcaps_len_step)\n",
    "            x_recon_step_all.append(x_recon_step)\n",
    "\n",
    "            if save_hooks:\n",
    "\n",
    "                # hook variables append\n",
    "                x_input_step_all.append(torch.stack(x_input_step, dim=1))\n",
    "                x_mask_step_all.append(torch.stack(x_mask_step, dim=1))\n",
    "                objcaps_step_all.append(torch.stack(objcaps_step, dim=1))\n",
    "\n",
    "                coups_step_all.append(torch.stack(coups_step, dim=1))\n",
    "                betas_step_all.append(torch.stack(betas_step, dim=1))\n",
    "                if rscores_step:\n",
    "                    rscores_step_all.append(torch.stack(rscores_step, dim=1))\n",
    "                if recon_coups_step:\n",
    "                    recon_coups_step_all.append(torch.stack(recon_coups_step, dim=1))\n",
    "                if outcaps_len_step:\n",
    "                    outcaps_len_step_all.append(torch.stack(outcaps_len_step, dim=1))\n",
    "                if outcaps_len_before_step:\n",
    "                    outcaps_len_before_step_all.append(torch.stack(outcaps_len_before_step, dim=1))\n",
    "\n",
    "                hook1.remove()\n",
    "                hook2.remove()        \n",
    "        \n",
    "        \n",
    "        i+=1\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "    # concat and add to outputs dictionary\n",
    "    x_all = torch.cat(x_all, dim=0)\n",
    "    y_all = torch.cat(y_all, dim=0)\n",
    "    if gtx:\n",
    "        gtx_all = torch.cat(gtx_all, dim=0)\n",
    "    else:\n",
    "        gtx_all = gtx\n",
    "    acc_all = torch.cat(acc_all, dim=0)\n",
    "    objcaps_len_step_all = torch.cat(objcaps_len_step_all, dim=0)\n",
    "    x_recon_step_all = torch.cat(x_recon_step_all, dim=0)\n",
    "    \n",
    "    if save_hooks:\n",
    "        outputs['x_input']= torch.cat(x_input_step_all, dim=0)\n",
    "        outputs['x_mask']= torch.cat(x_mask_step_all, dim=0)\n",
    "        outputs['objcaps']= torch.cat(objcaps_step_all, dim=0)\n",
    "\n",
    "        outputs['coups'] = torch.cat(coups_step_all, dim=0)\n",
    "        outputs['betas'] = torch.cat(betas_step_all, dim=0)\n",
    "        if rscores_step_all:\n",
    "            outputs['rscores'] = torch.cat(rscores_step_all, dim=0)\n",
    "        if recon_coups_step_all:\n",
    "            outputs['recon_coups'] = torch.cat(recon_coups_step_all, dim=0)\n",
    "        if outcaps_len_step_all:\n",
    "            outputs['outcaps_len'] = torch.cat(outcaps_len_step_all, dim=0)\n",
    "        if outcaps_len_before_step_all:\n",
    "            outputs['outcaps_len_before'] = torch.cat(outcaps_len_before_step_all, dim=0)\n",
    "            \n",
    "        return x_all, gtx_all, y_all, acc_all, objcaps_len_step_all, x_recon_step_all, outputs  \n",
    "\n",
    "    else:\n",
    "        return x_all, gtx_all, y_all, acc_all, objcaps_len_step_all, x_recon_step_all\n",
    "    \n",
    "    \n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_cnn_on_mnistc_original(corruption, cnn, max_batch_num=None):\n",
    "    path_images = os.path.join(PATH_MNISTC, corruption, 'test_images.npy')\n",
    "    path_labels = os.path.join(PATH_MNISTC, corruption, 'test_labels.npy')\n",
    "\n",
    "    # convert to torch\n",
    "    images = np.load(path_images)\n",
    "    labels = np.load(path_labels)\n",
    "    transform_tohot = T.Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "    images_tensorized = torch.stack([T.ToTensor()(im) for im in images])\n",
    "    labels_tensorized = torch.stack([transform_tohot(label) for label in labels])\n",
    "    # print(images_tensorized.shape) #torch.Size([10000, 1, 28, 28])\n",
    "    # print(labels_tensorized.shape) #torch.Size([10000, 10])\n",
    "\n",
    "    # create dataloader\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if DEVICE == 'cuda' else {}\n",
    "    dataset = TensorDataset(images_tensorized, labels_tensorized)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCHSIZE, shuffle=False, drop_last=False, **kwargs)\n",
    "\n",
    "    # save output\n",
    "    x_all, y_all, pred_all, acc_all, class_prob_all = [],[],[], [],[]\n",
    "    cnn.eval() \n",
    "\n",
    "    # get input and gt\n",
    "    i=0\n",
    "    for data in dataloader:\n",
    "        \n",
    "#         if max_batch_num:\n",
    "#             if i == max_batch_num:\n",
    "#                 break\n",
    "\n",
    "        if i == max_batch_num:\n",
    "            x, y = data\n",
    "            gtx = None\n",
    "                    \n",
    "            data, target = x.to(DEVICE),  y.to(DEVICE)\n",
    "            target = target.argmax(dim=1, keepdim=True)\n",
    "            output = cnn(data)\n",
    "            #                 test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            acc = pred.eq(target.view_as(pred))\n",
    "\n",
    "            x_all.append(data)\n",
    "            y_all.append(target.flatten())\n",
    "            pred_all.append(pred.flatten())\n",
    "            acc_all.append(acc.flatten().float())\n",
    "            class_prob_all.append(output)\n",
    "        \n",
    "        i+=1\n",
    "\n",
    "\n",
    "    # concat and add to outputs dictionary\n",
    "    x_all = torch.cat(x_all, dim=0)\n",
    "    y_all = torch.cat(y_all, dim=0)\n",
    "    pred_all = torch.cat(pred_all, dim=0)\n",
    "    acc_all = torch.cat(acc_all, dim=0)\n",
    "    class_prob_all = torch.cat(class_prob_all, dim=0)\n",
    "\n",
    "    return x_all, y_all, class_prob_all, pred_all, acc_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a182a2",
   "metadata": {},
   "source": [
    "# Evaluate on batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c5c6a1c-c974-4a55-825b-7c12fae938b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== model instantiated like below: =============\n",
      "TASK: mnist_recon_low (# targets: 1, # classes: 10, # background: 0)\n",
      "TIMESTEPS #: 5\n",
      "ENCODER: resnet w/ None projection\n",
      "...resulting primary caps #: 288, dim: 8\n",
      "ROUTINGS # 3\n",
      "Object #: 10, BG Capsule #: 0\n",
      "DECODER: fcn, w/ None projection\n",
      "...recon only one object capsule: True\n",
      "...use recon mask for attention: True\n",
      "...with mask type bool, threshold 0.1, apply_method match\n",
      "========================================================\n",
      "\n",
      "model is loaded from ./results/mnist/Aug14_0524_lsf_res4_run2/best_epoch29_acc1.0000.pt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corruption</th>\n",
       "      <th>model1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>identity</td>\n",
       "      <td>0.993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shot_noise</td>\n",
       "      <td>0.974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>impulse_noise</td>\n",
       "      <td>0.968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>glass_blur</td>\n",
       "      <td>0.943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>motion_blur</td>\n",
       "      <td>0.957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>shear</td>\n",
       "      <td>0.989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>scale</td>\n",
       "      <td>0.962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rotate</td>\n",
       "      <td>0.934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>brightness</td>\n",
       "      <td>0.993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>translate</td>\n",
       "      <td>0.592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stripe</td>\n",
       "      <td>0.654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>fog</td>\n",
       "      <td>0.963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>spatter</td>\n",
       "      <td>0.982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dotted_line</td>\n",
       "      <td>0.987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>zigzag</td>\n",
       "      <td>0.945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>canny_edges</td>\n",
       "      <td>0.719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       corruption  model1\n",
       "1        identity   0.993\n",
       "2      shot_noise   0.974\n",
       "3   impulse_noise   0.968\n",
       "4      glass_blur   0.943\n",
       "5     motion_blur   0.957\n",
       "6           shear   0.989\n",
       "7           scale   0.962\n",
       "8          rotate   0.934\n",
       "9      brightness   0.993\n",
       "10      translate   0.592\n",
       "11         stripe   0.654\n",
       "12            fog   0.963\n",
       "13        spatter   0.982\n",
       "14    dotted_line   0.987\n",
       "15         zigzag   0.945\n",
       "16    canny_edges   0.719"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>average</th>\n",
       "      <td>0.909688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shape</th>\n",
       "      <td>0.964875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model1\n",
       "average  0.909688\n",
       "shape    0.964875"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##################\n",
    "# model comparison on a single batch \n",
    "# (if tested on mnist_c_mini, all different corruption type batches are tested)\n",
    "##################\n",
    "\n",
    "CORRUPTION_TYPE_INTEREST = [\n",
    "         'glass_blur','motion_blur', 'impulse_noise','shot_noise',\n",
    "        'fog','dotted_line','spatter', 'zigzag']\n",
    "\n",
    "task='mnist_c_mini' # 'mnist_occlusion', 'mnist_recon'\n",
    "train=False #train or test dataset\n",
    "print_args=False\n",
    "args_to_update = {'device':DEVICE, 'batch_size':BATCHSIZE, \n",
    "                 'time_steps': 5, 'routings':3 , 'mask_threshold': 0.1}\n",
    "\n",
    "\n",
    "modellist = [\n",
    "\n",
    "# original rrcapsnet (mnist_recon, trainvalsplit)\n",
    "# './models/rrcapsnet/run1_epoch50_acc0.9917.pt',\n",
    "# './models/rrcapsnet/run2_epoch50_acc0.9915.pt',\n",
    "# './models/rrcapsnet/run3_epoch50_acc0.9907.pt',\n",
    "# './models/rrcapsnet/run4_epoch50_acc0.9905.pt',\n",
    "# './models/rrcapsnet/run5_epoch50_acc0.9907.pt',\n",
    "\n",
    "# './results/mnist/Apr28_1815_capsnetencoder_cycle/archive_model_epoch220_acc0.9961.pt'\n",
    "# './results/mnist/Apr28_1815_capsnetencoder_cycle/archive_model_epoch390_acc0.9992.pt'\n",
    "# './results/mnist/Apr28_2001_shift_adamclr/archive_model_epoch50_acc0.9860.pt'\n",
    "# './results/mnist/Apr28_2055_shift_adamclr_epoch1000/best_model_epoch147_acc0.9885.pt'\n",
    "# './results/mnist/Apr28_2131_shift_adamclr_epoch1000_difflrrange/archive_model_epoch80_acc0.9979.pt'\n",
    "# './results/mnist/Apr29_0120_test/archive_epoch50_acc0.9975.pt'\n",
    "\n",
    "# clean-wd\n",
    "# './results/mnist/Apr29_0213_clean_wd_run1/best_epoch75_acc0.9981.pt',\n",
    "\n",
    "# best our\n",
    "# './models/rrcapsnet/rrcapsnet_best.pt'\n",
    "    \n",
    "# clean-aug\n",
    "# './results/mnist/May25_0411_clean_aug_run3/best_epoch99_acc0.9713.pt'\n",
    "    \n",
    "# recon edge\n",
    "# './results/mnist/May25_2049_recon_edge_run1/best_epoch68_acc0.9980.pt'\n",
    "# './results/mnist/May25_2111_recon_edge_run2/best_epoch66_acc0.9986.pt'\n",
    "# './results/mnist/May25_2156_train_edge_run1/archive_epoch70_acc0.9955.pt'    \n",
    "# './results/mnist/May25_2219_train_edge_run2/best_epoch39_acc0.9970.pt'\n",
    "# './results/mnist/May25_2131_recon_edge_run3/best_epoch70_acc0.9987.pt'\n",
    "# blur-resnet\n",
    "# './results/mnist/May26_0133_test/best_epoch51_acc0.9999.pt'\n",
    "# './results/mnist/May27_0156_test2/best_epoch33_acc0.9999.pt'\n",
    "# './results/mnist/May28_2148_test3/best_epoch19_acc1.0000.pt'\n",
    "# './results/mnist/May28_2152_test-same/best_epoch17_acc1.0000.pt'\n",
    "# './results/mnist/May29_0142_test4/best_epoch21_acc1.0000.pt'\n",
    "# './results/mnist/May29_0220_test5-32/best_epoch26_acc1.0000.pt'\n",
    "# './results/mnist/May29_0210_test5/best_epoch23_acc1.0000.pt'\n",
    "# './results/mnist/May29_1445_test4-32/best_epoch29_acc1.0000.pt'\n",
    "# './results/mnist/May29_1525_test6/best_epoch32_acc1.0000.pt'\n",
    "# './results/mnist/May29_1543_test6-64/best_epoch38_acc1.0000.pt'\n",
    "# './results/mnist/May29_2031_test7/best_epoch29_acc1.0000.pt'\n",
    "\n",
    "# './results/mnist/May29_2045_blur_res4_run1/best_epoch32_acc1.0000.pt',\n",
    "    \n",
    "'./results/mnist/Aug14_0524_lsf_res4_run2/best_epoch29_acc1.0000.pt'\n",
    "]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for i, load_model_path in enumerate(modellist):\n",
    "    \n",
    "    # load model\n",
    "    args = load_args(load_model_path, args_to_update, print_args)\n",
    "    model = load_model(args)\n",
    "    print(f'model is loaded from {load_model_path}')\n",
    "\n",
    "    if task=='mnist_c_mini':\n",
    "        df['corruption'] = CORRUPTION_TYPES\n",
    "        accs = []\n",
    "        for corruption in CORRUPTION_TYPES:\n",
    "            x, gtx, y, acc_all, objcaps_len_step, x_recon_step = \\\n",
    "            evaluate_model_on_mnistc_mini(corruption, model, args, train, verbose=False, save_hooks=False)\n",
    "            accs.append(acc_all.mean().item())\n",
    "            \n",
    "\n",
    "            \n",
    "    else:\n",
    "        accs = evaluate_on_batch(task, 1, model, args, train, verbose=False, onlyacc=True)\n",
    "\n",
    "    df[f'model{i+1}'] =accs\n",
    "\n",
    "df.index = np.arange(1, len(df)+1)\n",
    "\n",
    "\n",
    "df_summary = pd.DataFrame()\n",
    "for i, load_model_path in enumerate(modellist):\n",
    "    df_summary.loc['average', f'model{i+1}'] = df.loc[:,f'model{i+1}'].mean()\n",
    "    df_summary.loc['shape', f'model{i+1}'] = df.loc[df['corruption'].isin(CORRUPTION_TYPE_INTEREST),f'model{i+1}'].mean()\n",
    "\n",
    "display(df)\n",
    "display(df_summary)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21737583",
   "metadata": {},
   "source": [
    "# Visualize individual, stepwise sample output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "051e114d-dcc8-41bd-b233-288cda89b355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== model instantiated like below: =============\n",
      "TASK: mnist_recon_low (# targets: 1, # classes: 10, # background: 0)\n",
      "TIMESTEPS #: 5\n",
      "ENCODER: resnet w/ None projection\n",
      "...resulting primary caps #: 288, dim: 8\n",
      "ROUTINGS # 3\n",
      "Object #: 10, BG Capsule #: 0\n",
      "DECODER: fcn, w/ None projection\n",
      "...recon only one object capsule: True\n",
      "...use recon mask for attention: True\n",
      "...with mask type bool, threshold 0.1, apply_method match\n",
      "========================================================\n",
      "\n",
      "==> corruption type: stripe, this batch acc: 0.6580000519752502\n",
      "\n",
      "======== best CNN model was loaded ========\n",
      "==> corruption type: stripe, this batch acc: 0.9580000638961792\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# arguments to change\n",
    "#############################\n",
    "\n",
    "# task and dataset\n",
    "task='mnist_c_mini' #'mnist_c_mini', 'mnist_recon', 'mnist_c_original'\n",
    "\n",
    "train=False #train or test dataset\n",
    "\n",
    "corruption_index =11\n",
    "corruption =CORRUPTION_TYPES[corruption_index-1]\n",
    "\n",
    "# model and args load\n",
    "print_args=False\n",
    "args_to_update = {'device':DEVICE, 'batch_size':BATCHSIZE, \n",
    "                 'time_steps': 5, 'routings': 3, 'mask_threshold': 0.1}\n",
    "\n",
    "load_model_path = './models/rrcapsnet/rrcapsnet_best.pt'\n",
    "load_model_path = './results/mnist/May25_2049_recon_edge_run1/best_epoch68_acc0.9980.pt'\n",
    "load_model_path= './results/mnist/May25_2131_recon_edge_run3/best_epoch70_acc0.9987.pt'\n",
    "load_model_path = './results/mnist/May29_2045_blur_res4_run1/best_epoch32_acc1.0000.pt'\n",
    "load_model_path = './results/mnist/Aug14_0524_lsf_res4_run2/best_epoch29_acc1.0000.pt'\n",
    "\n",
    "# model comparison\n",
    "compare_with_cnn = True\n",
    "\n",
    "###############################\n",
    "# load model and model output\n",
    "###############################\n",
    "args = load_args(load_model_path, args_to_update, print_args)\n",
    "model = load_model(args)\n",
    "\n",
    "if task == 'mnist_c_mini':\n",
    "    x, gtx, y_hot, acc_model, objcaps_len_step, x_recon_step, outputs_model  = \\\n",
    "    evaluate_model_on_mnistc_mini(corruption, model, args, train, verbose=False, save_hooks=True, max_batch_num=None)\n",
    "    print(f'==> corruption type: {corruption}, this batch acc: {acc_model.mean().item()}')\n",
    "elif task =='mnist_c_original':\n",
    "    print(\"original is used\")\n",
    "    batchnum=0\n",
    "    x, gtx, y_hot, acc_model, objcaps_len_step, x_recon_step, outputs_model = \\\n",
    "    evaluate_model_on_mnistc_original(corruption, model, verbose=False, save_hooks=True, max_batch_num=batchnum)\n",
    "    print(f'==> corruption type: {corruption}, this batch acc: {acc_model.mean().item()}')\n",
    "else:\n",
    "#     batchnum=corruption_index*int(1000/BATCHSIZE)-1\n",
    "    x, gtx, y, objcaps_len_step, x_recon_step, outputs_model = evaluate_on_batch(task, 1, model, args, train)\n",
    "\n",
    "#################################    \n",
    "# model prediction for computing accuracy\n",
    "##################################\n",
    "objcaps_len_step_narrow = objcaps_len_step.narrow(dim=2,start=0, length=args.num_classes)\n",
    "# pred_model = objcaps_len_step_narrow.max(dim=-1)[1][:,-1] #torch.Size([1000, 3])\n",
    "\n",
    "if ACC_TYPE=='hypothesis':\n",
    "    if args.time_steps==1:\n",
    "        y_pred = objcaps_len_step_narrow[:,-1]\n",
    "        accs = topkacc(y_pred, y_true, topk=1)\n",
    "    else:\n",
    "        acc_model_check, pred_model, nstep  = compute_hypothesis_based_acc(objcaps_len_step_narrow, y_hot, only_acc=False)\n",
    "    \n",
    "elif ACC_TYPE == 'entropy':    \n",
    "    if args.time_steps==1:\n",
    "        y_pred = objcaps_len_step_narrow[:,-1]\n",
    "        accs = topkacc(y_pred, y_true, topk=1)\n",
    "    else: \n",
    "        acc_model_check, pred_model, nstep, no_stop_condition, entropy_model =compute_entropy_based_acc(objcaps_len_step_narrow, y_hot, threshold=0.6, use_cumulative = False, only_acc= False)\n",
    "\n",
    "assert round(acc_model.mean().item(), 4) == round(acc_model_check.float().mean().item(), 4)\n",
    "\n",
    "\n",
    "############################################\n",
    "# if compare with cnn results\n",
    "############################################\n",
    "\n",
    "if compare_with_cnn:\n",
    "    from train_cnn import Net\n",
    "\n",
    "    path_cnn = './models/cnn/cnn_best.pt'\n",
    "\n",
    "    cnn = Net().to(DEVICE)\n",
    "    cnn.load_state_dict(torch.load(path_cnn))\n",
    "    cnn.eval()\n",
    "\n",
    "    print('\\n======== best CNN model was loaded ========')\n",
    "    if task == 'mnist_c_mini':\n",
    "        data_cnn, target_cnn, logsoft_cnn, pred_cnn, acc_cnn \\\n",
    "        = evaluate_cnn_on_mnistc_mini(corruption, cnn, max_batch_num=None)\n",
    "        print(f'==> corruption type: {corruption}, this batch acc: {acc_cnn.float().mean().item()}')\n",
    "    elif task =='mnist_c_original':\n",
    "        print(\"original is used\")\n",
    "        data_cnn, target_cnn, logsoft_cnn, pred_cnn, acc_cnn \\\n",
    "        =  evaluate_cnn_on_mnistc_original(corruption, cnn,  max_batch_num=batchnum)\n",
    "        print(f'==> corruption type: {corruption}, this batch acc: {acc_cnn.float().mean().item()}')\n",
    "    \n",
    "    pred_to_compare = pred_cnn\n",
    "    #######################\n",
    "    # get trials id where both model fails & disagree\n",
    "    #######################\n",
    "    assert (target_cnn.cpu() == y_hot.max(dim=1)[1].cpu()).all()\n",
    "    bool_bothincorrect = ~(acc_model.bool())* ~(acc_cnn.bool())\n",
    "    bool_diffanswer = (pred_model!= pred_cnn)\n",
    "    bool_onlycnncorrect = ~(acc_model.bool())* (acc_cnn.bool())\n",
    "    # idx_bothincorrect = torch.nonzero(bool_bothincorrect)\n",
    "    trialid_to_visualize = torch.nonzero(bool_bothincorrect*bool_diffanswer).flatten().tolist()\n",
    "    # trialid_to_visualize = torch.nonzero(bool_onlycnncorrect*bool_diffanswer).flatten().tolist()\n",
    "\n",
    "    print(len(trialid_to_visualize))\n",
    "else:\n",
    "    pred_to_compare = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be6c94e-7fe8-4657-b1a8-4ba5f24072b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# stepwise visualization\n",
    "###################################\n",
    "idx_start = 0 # can't be higher than batchsize\n",
    "n_image =500\n",
    "\n",
    "## plot condition\n",
    "condition = 'first correct last incorrect'\n",
    "condition = 'first incorrect last correct'\n",
    "# condition = 'first incorrect last incorrect'\n",
    "condition = 'all'\n",
    "# condition = 'correct'\n",
    "condition = 'incorrect' \n",
    "\n",
    "## plot only specific categories\n",
    "# idx_category =0\n",
    "# boolidx = y_hot.argmax(dim=-1).numpy()==idx_category\n",
    "# condition = list(np.where(boolidx==True)[0])\n",
    "\n",
    "## plot different steps\n",
    "# n_step = 3\n",
    "# boolidx = nstep==n_step\n",
    "# condition = list(np.where(boolidx==True)[0])\n",
    "\n",
    "## plot by ids \n",
    "# condition = [6, 41, 66]\n",
    "# condition = [7, 11]\n",
    "# condition = [66, 114, 656] #batchnum1\n",
    "# condition = [6] #batchnum2\n",
    "# condition = [6] #batchnum3\n",
    "# condition = [17]\n",
    "# condition = [8] #batchnum 3\n",
    "# condition = [22, 33,41,44,75,82,88,147] #fog batch\n",
    "# condition = [6,7]\n",
    "# condition = [17]\n",
    "# condition = [447]\n",
    "# condition = [167, 502, 1466]\n",
    "\n",
    "# condition = trialid_to_visualize #[:20]\n",
    "# condition = list(np.where(nstep==3)[0])\n",
    "# condition = [309]\n",
    "# condition = [560, 893]\n",
    "# condition=[7]\n",
    "# condition=[82, 92]\n",
    "\n",
    "# condition = [329,336]\n",
    "idx_plotted =\\\n",
    "visualize_detail(model, x.to(DEVICE), y_hot, outputs_model, \n",
    "                 x_recon_step, objcaps_len_step, args,\n",
    "                 start=idx_start, n_image=n_image, plot_trials_when = condition,\n",
    "                 plot_routings=False, \n",
    "                 pred_to_compare=pred_to_compare, \n",
    "                 num_steps_to_finish=list(nstep),\n",
    "                 entropy = entropy_model\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2224ef3d-c05c-4d10-847b-66c4ff646733",
   "metadata": {},
   "source": [
    "# Visualization on batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c89f6d-c9ae-4351-b946-36b0089860e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# if you need separate visualization for each output \n",
    "##################################\n",
    "idx_start = 40 # can't be higher than batchsize\n",
    "n_image =10\n",
    "\n",
    "print(\"\\n\\n=========================== reconstruction =================================\")\n",
    "visualize_batch(x, y, x_recon_step, objcaps_len_step, include_sum=False, start=idx_start, n_image=n_image)\n",
    "\n",
    "print(\"\\n\\n=========================== reconstruction mask =================================\")\n",
    "visualize_batch(x, y, outputs['x_mask'], objcaps_len_step, start=idx_start, n_image=n_image)\n",
    "\n",
    "print(\"\\n\\n=========================== masked input =================================\")\n",
    "visualize_batch(x, y, outputs['x_input'], objcaps_len_step, start=idx_start, n_image=n_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596b7a65-9e3c-4502-b99a-54e24a66c38c",
   "metadata": {},
   "source": [
    "# Archived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24d6d605-0e8e-4625-9ebd-1cd83399c650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== model instantiated like below: =============\n",
      "TASK: mnist_recon_low (# targets: 1, # classes: 10, # background: 0)\n",
      "TIMESTEPS #: 4\n",
      "ENCODER: resnet w/ None projection\n",
      "...resulting primary caps #: 288, dim: 8\n",
      "ROUTINGS # 3\n",
      "Object #: 10, BG Capsule #: 0\n",
      "DECODER: fcn, w/ None projection\n",
      "...recon only one object capsule: True\n",
      "...use recon mask for attention: True\n",
      "...with mask type bool, threshold 0.1, apply_method match\n",
      "========================================================\n",
      "\n",
      "==> corruption type: stripe, this batch acc: 0.6300000548362732\n",
      "\n",
      "======== best CNN model was loaded ========\n",
      "==> corruption type: stripe, this batch acc: 0.9580000638961792\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# #############################\n",
    "# # arguments to change\n",
    "# #############################\n",
    "\n",
    "# # task and dataset\n",
    "# task='mnist_c_mini' #'mnist_c_mini', 'mnist_recon', 'mnist_c_original'\n",
    "\n",
    "# train=False #train or test dataset\n",
    "\n",
    "# corruption_index =11\n",
    "# corruption =CORRUPTION_TYPES[corruption_index-1]\n",
    "\n",
    "# # model and args load\n",
    "# print_args=False\n",
    "# args_to_update = {'device':DEVICE, 'batch_size':BATCHSIZE, \n",
    "#                  'time_steps': 4, 'routings': 3, 'mask_threshold': 0.1}\n",
    "\n",
    "# load_model_path = './models/rrcapsnet/rrcapsnet_best.pt'\n",
    "# load_model_path = './results/mnist/May25_2049_recon_edge_run1/best_epoch68_acc0.9980.pt'\n",
    "# load_model_path= './results/mnist/May25_2131_recon_edge_run3/best_epoch70_acc0.9987.pt'\n",
    "# load_model_path = './results/mnist/May29_2045_blur_res4_run1/best_epoch32_acc1.0000.pt'\n",
    "# load_model_path = './results/mnist/Aug14_0524_lsf_res4_run2/best_epoch29_acc1.0000.pt'\n",
    "\n",
    "# # model comparison\n",
    "# compare_with_cnn = True\n",
    "\n",
    "# ###############################\n",
    "# # load model and model output\n",
    "# ###############################\n",
    "# args = load_args(load_model_path, args_to_update, print_args)\n",
    "# model = load_model(args)\n",
    "\n",
    "# if task == 'mnist_c_mini':\n",
    "#     x, gtx, y_hot, acc_model, objcaps_len_step, x_recon_step, outputs_model  = \\\n",
    "#     evaluate_model_on_mnistc_mini(corruption, model, args, train, verbose=False, save_hooks=True, max_batch_num=None)\n",
    "#     print(f'==> corruption type: {corruption}, this batch acc: {acc_model.mean().item()}')\n",
    "# elif task =='mnist_c_original':\n",
    "#     print(\"original is used\")\n",
    "#     batchnum=0\n",
    "#     x, gtx, y_hot, acc_model, objcaps_len_step, x_recon_step, outputs_model = \\\n",
    "#     evaluate_model_on_mnistc_original(corruption, model, verbose=False, save_hooks=True, max_batch_num=batchnum)\n",
    "#     print(f'==> corruption type: {corruption}, this batch acc: {acc_model.mean().item()}')\n",
    "# else:\n",
    "# #     batchnum=corruption_index*int(1000/BATCHSIZE)-1\n",
    "#     x, gtx, y, objcaps_len_step, x_recon_step, outputs_model = evaluate_on_batch(task, 1, model, args, train)\n",
    "\n",
    "# #################################    \n",
    "# # model prediction for computing accuracy\n",
    "# ##################################\n",
    "# objcaps_len_step_narrow = objcaps_len_step.narrow(dim=2,start=0, length=args.num_classes)\n",
    "# # pred_model = objcaps_len_step_narrow.max(dim=-1)[1][:,-1] #torch.Size([1000, 3])\n",
    "\n",
    "# if ACC_TYPE=='hypothesis':\n",
    "\n",
    "#     def get_nstep(objcaps_len_step_narrow, y_hot):\n",
    "#         def get_first_zero_index(x, axis=1):\n",
    "#             cond = (x == 0)\n",
    "#             return ((cond.cumsum(axis) == 1) & cond).max(axis, keepdim=True)[1]\n",
    "\n",
    "#         pstep = objcaps_len_step_narrow.max(dim=-1)[1]\n",
    "        \n",
    "#         # check whether consecutive predictions are the same (diff=0)\n",
    "#         pnow = pstep[:,1:]\n",
    "#         pbefore = pstep[:,:-1]\n",
    "#         pdiff = (pnow-pbefore)\n",
    "#         null_column = -99*torch.ones(pdiff.size(0),1).to(pdiff.device) # add one null column at start\n",
    "#         pdiff = torch.cat([null_column, pdiff], dim=1)\n",
    "#         pdiff[:,-1]=0 # add diff= 0 to final step (to use final prediction if no criterion made)\n",
    "        \n",
    "#         # get nstep required and model predictions\n",
    "#         nstep = get_first_zero_index(pdiff)\n",
    "#         pred_model= torch.gather(pstep, 1, nstep).flatten()\n",
    "#         acc_model = torch.eq(pred_model.cpu(), y_hot.max(dim=1)[1])\n",
    "#         return (nstep.flatten()+1).cpu().numpy(), pred_model, acc_model\n",
    "\n",
    "#     nstep, pred_model, acc_model_check = get_nstep(objcaps_len_step_narrow, y_hot)\n",
    "    \n",
    "# elif ACC_TYPE == 'entropy':\n",
    "#     from torch.distributions import Categorical\n",
    "    \n",
    "#     def get_nstep(objcaps_len_step_narrow, y_hot, threshold=1.0, use_cumulative = True):\n",
    "\n",
    "#         def get_first_true_index(boolarray, axis=1, when_no_true='final_index'):\n",
    "#             # boolarray = Batch x Stepsize\n",
    "#             first_true_index = ((boolarray.cumsum(axis) == 1) & boolarray).max(axis, keepdim=True)[1] # when no true, set as 0\n",
    "\n",
    "#             if when_no_true == 'final_index': # when there is no true, use final index\n",
    "#                 final_index = boolarray.shape[1]-1\n",
    "#                 no_true_condition = (~boolarray).all(dim=1).reshape(-1,1)\n",
    "#                 first_true_index = first_true_index + final_index * no_true_condition\n",
    "#                 return  first_true_index, no_true_condition\n",
    "#             else:\n",
    "#                 return first_true_index\n",
    "\n",
    "#         if use_cumulative:\n",
    "#             score = objcaps_len_step_narrow.cumsum(dim=1)\n",
    "#             pred = score.max(dim=-1)[1]\n",
    "#         else:\n",
    "#             score = objcaps_len_step_narrow # Batch x Stepsize x Category\n",
    "#             pred = score.max(dim=-1)[1] # Batch x Stepsize\n",
    "\n",
    "\n",
    "#         # compute entropy from softmax output with Temp scale\n",
    "#         T=0.2\n",
    "#         softmax = F.softmax(score/T, dim=-1) # torch.Size([1000, 4, 10])\n",
    "#         entropy = Categorical(probs = softmax).entropy() # torch.Size([1000, 4])\n",
    "\n",
    "#         # entropy thresholding\n",
    "#         stop = entropy<threshold\n",
    "#         boolarray = (stop == True)\n",
    "\n",
    "#         # get first index that reached threshold\n",
    "#         first_true_index, no_stop_condition = get_first_true_index(boolarray, axis=1, when_no_true='final_index')\n",
    "\n",
    "#         final_pred = torch.gather(pred, dim=1, index= first_true_index).flatten()\n",
    "#         acc = torch.eq(final_pred.cpu(), y_hot.max(dim=1)[1])\n",
    "#         nstep = (first_true_index.flatten()+1).cpu().numpy()\n",
    "\n",
    "#         return nstep, final_pred, acc, no_stop_condition\n",
    "\n",
    "#     nstep, pred_model, acc_model_check, no_stop_condition = get_nstep(objcaps_len_step_narrow, y_hot, threshold=1.0, use_cumulative = False)\n",
    "\n",
    "# assert round(acc_model.mean().item(), 4) == round(acc_model_check.float().mean().item(), 4)\n",
    "\n",
    "\n",
    "# ############################################\n",
    "# # if compare with cnn results\n",
    "# ############################################\n",
    "\n",
    "# if compare_with_cnn:\n",
    "#     from train_cnn import Net\n",
    "\n",
    "#     path_cnn = './models/cnn/cnn_best.pt'\n",
    "\n",
    "#     cnn = Net().to(DEVICE)\n",
    "#     cnn.load_state_dict(torch.load(path_cnn))\n",
    "#     cnn.eval()\n",
    "\n",
    "#     print('\\n======== best CNN model was loaded ========')\n",
    "#     if task == 'mnist_c_mini':\n",
    "#         data_cnn, target_cnn, logsoft_cnn, pred_cnn, acc_cnn \\\n",
    "#         = evaluate_cnn_on_mnistc_mini(corruption, cnn, max_batch_num=None)\n",
    "#         print(f'==> corruption type: {corruption}, this batch acc: {acc_cnn.float().mean().item()}')\n",
    "#     elif task =='mnist_c_original':\n",
    "#         print(\"original is used\")\n",
    "#         data_cnn, target_cnn, logsoft_cnn, pred_cnn, acc_cnn \\\n",
    "#         =  evaluate_cnn_on_mnistc_original(corruption, cnn,  max_batch_num=batchnum)\n",
    "#         print(f'==> corruption type: {corruption}, this batch acc: {acc_cnn.float().mean().item()}')\n",
    "    \n",
    "#     pred_to_compare = pred_cnn\n",
    "#     #######################\n",
    "#     # get trials id where both model fails & disagree\n",
    "#     #######################\n",
    "#     assert (target_cnn.cpu() == y_hot.max(dim=1)[1].cpu()).all()\n",
    "#     bool_bothincorrect = ~(acc_model.bool())* ~(acc_cnn.bool())\n",
    "#     bool_diffanswer = (pred_model!= pred_cnn)\n",
    "#     bool_onlycnncorrect = ~(acc_model.bool())* (acc_cnn.bool())\n",
    "#     # idx_bothincorrect = torch.nonzero(bool_bothincorrect)\n",
    "#     trialid_to_visualize = torch.nonzero(bool_bothincorrect*bool_diffanswer).flatten().tolist()\n",
    "#     # trialid_to_visualize = torch.nonzero(bool_onlycnncorrect*bool_diffanswer).flatten().tolist()\n",
    "\n",
    "#     print(len(trialid_to_visualize))\n",
    "# else:\n",
    "#     pred_to_compare = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09befc5-86b1-4909-9bbe-fc7b91a427b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################\n",
    "# # test a single model, and visualize outputs\n",
    "# #############################\n",
    "# task='mnist_c_mini'\n",
    "# task='mnist_recon'\n",
    "# task='mnist_c_original'\n",
    "\n",
    "# train=False #train or test dataset\n",
    "# print_args=False\n",
    "\n",
    "# # load model \n",
    "# args_to_update = {'device':DEVICE, 'batch_size':BATCHSIZE, \n",
    "#                  'time_steps': 4, 'routings': 3, 'mask_threshold': 0.1}\n",
    "\n",
    "# load_model_path = './results/mnist/May29_2045_blur_res4_run1/best_epoch32_acc1.0000.pt'\n",
    "\n",
    "# args = load_args(load_model_path, args_to_update, print_args)\n",
    "# model = load_model(args)\n",
    "\n",
    "\n",
    "# # obtain model prediction and outputs\n",
    "# # corruption_index =2\n",
    "# corruption =CORRUPTION_TYPES[corruption_index-1]\n",
    "\n",
    "# if task == 'mnist_c_mini':\n",
    "#     x, gtx, y_hot, acc_model, objcaps_len_step, x_recon_step, outputs_model  = \\\n",
    "#     evaluate_model_on_mnistc_mini(corruption, model, args, train, verbose=False, save_hooks=True, max_batch_num=None)\n",
    "#     print(f'==> corruption type: {corruption}, this batch acc: {acc_model.mean().item()}')\n",
    "# elif task =='mnist_c_original':\n",
    "#     print(\"original is used\")\n",
    "#     batchnum=0\n",
    "#     x, gtx, y_hot, acc_model, objcaps_len_step, x_recon_step, outputs_model = \\\n",
    "#     evaluate_model_on_mnistc_original(corruption, model, verbose=False, save_hooks=True, max_batch_num=batchnum)\n",
    "#     print(f'==> corruption type: {corruption}, this batch acc: {acc_model.mean().item()}')\n",
    "# else:\n",
    "# #     batchnum=corruption_index*int(1000/BATCHSIZE)-1\n",
    "#     x, gtx, y, objcaps_len_step, x_recon_step, outputs_model = evaluate_on_batch(task, 1, model, args, train)\n",
    "\n",
    "# # get model prediction\n",
    "# objcaps_len_step_narrow = objcaps_len_step.narrow(dim=2,start=0, length=args.num_classes)\n",
    "# # pred_model = objcaps_len_step_narrow.max(dim=-1)[1][:,-1] #torch.Size([1000, 3])\n",
    "\n",
    "\n",
    "# def get_nstep(objcaps_len_step_narrow, y_hot):\n",
    "#     def get_first_zero_index(x, axis=1):\n",
    "#         cond = (x == 0)\n",
    "#         return ((cond.cumsum(axis) == 1) & cond).max(axis, keepdim=True)[1]\n",
    "\n",
    "#     pstep = objcaps_len_step_narrow.max(dim=-1)[1]\n",
    "#     pnow = pstep[:,1:]\n",
    "#     pbefore = pstep[:,:-1]\n",
    "\n",
    "#     pdiff = (pnow-pbefore)\n",
    "#     null_column = -99*torch.ones(pdiff.size(0),1).to(pdiff.device)\n",
    "#     pdiff = torch.cat([null_column, pdiff], dim=1)\n",
    "#     pdiff[:,-1]=0\n",
    "#     nstep = get_first_zero_index(pdiff)\n",
    "#     pred_model= torch.gather(pstep, 1, nstep).flatten()\n",
    "#     acc_model = torch.eq(pred_model.cpu(), y_hot.max(dim=1)[1])\n",
    "#     return (nstep.flatten()+1).cpu().numpy(), pred_model, acc_model\n",
    "\n",
    "# nstep, pred_model, acc_model_check = get_nstep(objcaps_len_step_narrow, y_hot)\n",
    "# assert round(acc_model.mean().item(), 4) == round(acc_model_check.float().mean().item(), 4)\n",
    "# # routing\n",
    "# ###################################\n",
    "# # stepwise visualization\n",
    "# ###################################\n",
    "# idx_start = 0 # can't be higher than batchsize\n",
    "# n_image =500\n",
    "\n",
    "\n",
    "\n",
    "# condition = 'first correct last incorrect'\n",
    "# condition = 'first incorrect last correct'\n",
    "# # condition = 'first incorrect last incorrect'\n",
    "# condition = 'all'\n",
    "# # condition = 'correct'\n",
    "# condition = 'incorrect' \n",
    "\n",
    "# condition = idx_plotted\n",
    "# visualize_detail(model, x.to(DEVICE), y_hot, outputs_model, \n",
    "#                  x_recon_step, objcaps_len_step, args,\n",
    "#                  start=idx_start, n_image=n_image, plot_trials_when = condition,\n",
    "#                  plot_routings=True, \n",
    "#                  pred_to_compare=pred_cnn, num_steps_to_finish=list(nstep), \n",
    "#                  only_plot_object=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c487bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot and save image outputs\n",
    "# from torchvision.utils import save_image, make_grid\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# n_image = 50\n",
    "# plot_save_dir = None\n",
    "# img_grid = torch.cat((x[:n_image].to(args.device),torch.sum(x_recon_step,dim=1)[:n_image]))\n",
    "# img_grid = make_grid(img_grid, nrow=10, padding=1, normalize=False, pad_value=1)\n",
    "\n",
    "# if plot_save_dir:\n",
    "#     model_name = args.log_dir.split('/')[-1]\n",
    "#     save_name = f'final_reconstruction_{model_name}.png'\n",
    "#     save_image(img_grid, os.path.join(plot_save_dir, save_name))\n",
    "\n",
    "# # img_g = Image.open(os.path.join(args.output_dir, save_name))\n",
    "# # f = plt.figure()\n",
    "# # f.set_figheight(15)\n",
    "# # f.set_figwidth(15)\n",
    "# plt.imshow(img_grid.cpu()[], cmap='gray_r') #matplot imshow() WILL AUTO CHANGE ONE CHANNEL IMG TO COLOR ONE\n",
    "# plt.show()\n",
    "\n",
    "# def show_reconstruction(model, test_loader, n_images, args):\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     from utils import combine_images\n",
    "#     from PIL import Image\n",
    "#     import numpy as np\n",
    "\n",
    "#     model.eval()\n",
    "#     for x, _ in test_loader:\n",
    "#         x = Variable(x[:min(n_images, x.size(0))].cuda(), volatile=True)\n",
    "#         _, x_recon = model(x)\n",
    "#         data = np.concatenate([x.data, x_recon.data])\n",
    "#         img = combine_images(np.transpose(data, [0, 2, 3, 1]))\n",
    "#         image = img * 255\n",
    "#         Image.fromarray(image.astype(np.uint8)).save(args.save_dir + \"/real_and_recon.png\")\n",
    "#         print()\n",
    "#         print('Reconstructed images are saved to %s/real_and_recon.png' % args.save_dir)\n",
    "#         print('-' * 70)\n",
    "#         plt.imshow(plt.imread(args.save_dir + \"/real_and_recon.png\", ))\n",
    "#         plt.show()\n",
    "#         break\n",
    "        \n",
    "# # https://medium.com/@sergioalves94/deep-learning-in-pytorch-with-cifar-10-dataset-858b504a6b54\n",
    "# from torchvision.utils import make_grid\n",
    "\n",
    "# for images, _ in train_loader:\n",
    "#     print('images.shape:', images.shape)\n",
    "#     plt.figure(figsize=(16,8))\n",
    "#     plt.axis('off')\n",
    "#     plt.imshow(make_grid(images, nrow=16).permute((1, 2, 0)))\n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

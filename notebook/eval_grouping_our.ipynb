{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current directory: /home/young/workspace/reconstruction/recon-mnistc\n"
     ]
    }
   ],
   "source": [
    "# set current directory (where this repo is located)\n",
    "import os\n",
    "PROJECT_ROOT = '/home/young/workspace/reconstruction/recon-mnistc'\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print('current directory:', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# load required libraries & modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import pprint\n",
    "import time\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from utils import *\n",
    "from loaddata import *\n",
    "from visualization import *\n",
    "from ourmodel import *\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "\n",
    "DATA_DIR = './data'\n",
    "DEVICE = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "# DEVICE = torch.device('cpu')\n",
    "\n",
    "BATCHSIZE = 1000\n",
    "ACC_TYPE = \"entropy\"\n",
    "\n",
    "# general helper funtions for model testing\n",
    "def load_model(args):\n",
    "    # load model\n",
    "    model = RRCapsNet(args).to(args.device) \n",
    "    model.load_state_dict(torch.load(args.load_model_path))\n",
    "    return model\n",
    "\n",
    "def load_args(load_model_path, args_to_update, verbose=False):\n",
    "    params_filename = os.path.dirname(load_model_path) + '/params.txt'\n",
    "    assert os.path.isfile(params_filename), \"No param flie exists\"\n",
    "    args = parse_params_wremove(params_filename, removelist = ['device']) \n",
    "    args = update_args(args, args_to_update)\n",
    "    args.load_model_path = load_model_path\n",
    "    if verbose:\n",
    "        pprint.pprint(args.__dict__, sort_dicts=False)\n",
    "    return args\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_our(task, model, args, train=False, verbose=True, onlyacc=False):\n",
    "    \n",
    "    # evaluate on one train/test batch \n",
    "    model.eval()\n",
    "    \n",
    "    dataloader = fetch_dataloader(task, DATA_DIR, DEVICE, BATCHSIZE, train)    \n",
    "    diter = iter(dataloader)\n",
    "\n",
    "\n",
    "    accs_all = []\n",
    "    for x, y in diter:\n",
    "        losses, accs, objcaps_len_step, x_recon_step = evaluate(model, x, y, args, acc_type=ACC_TYPE, gtx=gtx)\n",
    "        accs_all.append(accs)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"==> On this sigle test batch: test_loss=%.5f, test_loss_class=%.5f, test_loss_recon=%.5f, test_acc=%.4f\"\n",
    "                % (losses[0], losses[1], losses[2], acc))\n",
    "    \n",
    "    accs_all = torch.cat(accs_all, dim=0)\n",
    "    if onlyacc:\n",
    "        return accs_all\n",
    "    else:\n",
    "        return accs_all, x, y, objcaps_len_step, x_recon_step\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_output_on_batch(task, batchnum, model, args, train=False, verbose=True, onlyacc=False):\n",
    "    \n",
    "    # evaluate on one train/test batch \n",
    "    model.eval()\n",
    "    \n",
    "    if task == 'mnist_recon':\n",
    "        # for mnist recon data, it has erased input(x) and intact input (gtx)\n",
    "        if train:\n",
    "            dataloader, val_dataloader = fetch_dataloader(task, DATA_DIR, DEVICE, BATCHSIZE, train)\n",
    "        else:\n",
    "            dataloader = fetch_dataloader(task, DATA_DIR, DEVICE, BATCHSIZE, train)            \n",
    "        diter = iter(dataloader)\n",
    "        for i in range(batchnum):\n",
    "            x, gtx, y = next(diter)\n",
    "    else:\n",
    "        dataloader = fetch_dataloader(task, DATA_DIR, DEVICE, BATCHSIZE, train)    \n",
    "        diter = iter(dataloader)\n",
    "        for i in range(batchnum):\n",
    "            x, y = next(diter)\n",
    "            gtx = None\n",
    "            \n",
    "            \n",
    "    # attach forward hooks for intermediate outputs for visualizations\n",
    "    outputs = {}\n",
    "    \n",
    "    # from model main output\n",
    "    x_input_step = []; x_mask_step = []; objcaps_step = []\n",
    "    \n",
    "    # from model dynamic routing\n",
    "    coups_step = []; betas_step= []; rscores_step=[]; recon_coups_step=[] \n",
    "    outcaps_len_step=[]; outcaps_len_before_step=[]\n",
    "\n",
    "    def get_attention_outputs():\n",
    "        def hook(model, input, output):\n",
    "            x_mask_step.append(output[0].detach())\n",
    "            x_input_step.append(output[1].detach())\n",
    "        return hook\n",
    "\n",
    "    def get_capsule_outputs():\n",
    "        def hook(model, input, output):\n",
    "            objcaps_step.append(output[0].detach())\n",
    "            coups_step.append(torch.stack(output[1]['coups'], dim=1))\n",
    "            betas_step.append(torch.stack(output[1]['betas'], dim=1)) \n",
    "            if 'rscores' in output[1].keys():\n",
    "                rscores_step.append(torch.stack(output[1]['rscores'], dim=1))\n",
    "            if 'recon_coups' in output[1].keys():\n",
    "                recon_coups_step.append(torch.stack(output[1]['recon_coups'], dim=1))\n",
    "            if 'outcaps_len' in output[1].keys():\n",
    "                outcaps_len_step.append(torch.stack(output[1]['outcaps_len'], dim=1))\n",
    "            if 'outcaps_len_before' in output[1].keys():\n",
    "                outcaps_len_before_step.append(torch.stack(output[1]['outcaps_len_before'], dim=1))\n",
    "        return hook\n",
    "    \n",
    "\n",
    "    hook1 = model.input_window.register_forward_hook(get_attention_outputs())\n",
    "    hook2 = model.capsule_routing.register_forward_hook(get_capsule_outputs())\n",
    "    \n",
    "    # evaluate and detach hooks\n",
    "    losses, accs, objcaps_len_step, x_recon_step = evaluate(model, x, y, args, acc_type=ACC_TYPE, gtx=gtx)\n",
    "    hook1.remove()\n",
    "    hook2.remove()\n",
    "    \n",
    "    # add tensor outputs dictionary\n",
    "    outputs['x_input'] = torch.stack(x_input_step, dim=1)\n",
    "    outputs['x_mask'] = torch.stack(x_mask_step, dim=1)\n",
    "    outputs['objcaps'] = torch.stack(objcaps_step, dim=1)\n",
    "    \n",
    "    outputs['coups'] = torch.stack(coups_step, dim=1)\n",
    "    outputs['betas'] = torch.stack(betas_step, dim=1)\n",
    "    if rscores_step:\n",
    "        outputs['rscores'] = torch.stack(rscores_step, dim=1)\n",
    "    if recon_coups_step:\n",
    "        outputs['recon_coups'] = torch.stack(recon_coups_step, dim=1)\n",
    "    if outcaps_len_step:\n",
    "        outputs['outcaps_len'] = torch.stack(outcaps_len_step, dim=1)\n",
    "    if outcaps_len_before_step:\n",
    "        outputs['outcaps_len_before'] = torch.stack(outcaps_len_before_step, dim=1)        \n",
    "    if verbose:\n",
    "        print(\"==> On this sigle test batch: test_loss=%.5f, test_loss_class=%.5f, test_loss_recon=%.5f, test_acc=%.4f\"\n",
    "              % (losses[0], losses[1], losses[2], acc))\n",
    "    \n",
    "    if onlyacc:\n",
    "        return accs\n",
    "    else:\n",
    "        return x, gtx, y, objcaps_len_step, x_recon_step, outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load model and run on batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== model instantiated like below: =============\n",
      "TASK: mnist_multi_high (# targets: 2, # classes: 10, # background: 0)\n",
      "TIMESTEPS #: 1\n",
      "ENCODER: resnet w/ None projection\n",
      "...resulting primary caps #: 512, dim: 8\n",
      "ROUTINGS # 1\n",
      "Object #: 10, BG Capsule #: 0\n",
      "DECODER: fcn, w/ None projection\n",
      "...recon only one object capsule: True\n",
      "========================================================\n",
      "\n",
      "model is loaded from ./results/mnist/Feb22_1712_mnist_multi_high/best_epoch50_acc1.0000.pt\n",
      "mnist_highoverlap_4pix_nodup_1fold_36width_2obj_train.pt mnist_highoverlap_4pix_nodup_1fold_36width_2obj_test.pt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>obj_acc</th>\n",
       "      <td>0.974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_acc</th>\n",
       "      <td>0.974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         model1\n",
       "obj_acc   0.974\n",
       "img_acc   0.974"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##################\n",
    "# model comparison on a single batch \n",
    "##################\n",
    "\n",
    "task='mnist_multi_high' # 'mnist_multi_high', 'mnist_recon'\n",
    "train=False #train or test dataset\n",
    "print_args=False\n",
    "args_to_update = {'device':DEVICE, 'batch_size':BATCHSIZE, \n",
    "                 'time_steps': 1, 'routings':1 , 'mask_threshold': 0.1}\n",
    "\n",
    "\n",
    "modellist = [\n",
    "    # './results/mnist/Feb22_0507_mnist_multi/best_epoch36_acc1.0000.pt',\n",
    "    './results/mnist/Feb22_1712_mnist_multi_high/best_epoch50_acc1.0000.pt',\n",
    "             ]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for i, load_model_path in enumerate(modellist):\n",
    "    \n",
    "    # load model\n",
    "    args = load_args(load_model_path, args_to_update, print_args)\n",
    "    model = load_model(args)\n",
    "    print(f'model is loaded from {load_model_path}')\n",
    "    batchnum = 1\n",
    "    obj_accs = evaluate_on_batch(task, batchnum, model, args, train, verbose=False, onlyacc=True)\n",
    "    img_accs = (obj_accs == 1).float()\n",
    "    df[f'model{i+1}'] = [obj_accs.mean().item(), img_accs.mean().item()]\n",
    "\n",
    "df.index = ['obj_acc', 'img_acc']\n",
    "\n",
    "display(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model1\n",
       "1    0.99"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "objcaps_len_step_narrow.size(1)\n",
    "\n",
    "df[f'model{i+1}'] = [accs.mean().item()]\n",
    "\n",
    "df.index = np.arange(1, len(df)+1)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9900000691413879"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ora-recog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
